<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/star.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"xinjieinformatik.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":"ture","show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":{"enable":true,"onlypost":false},"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="决策树 GBDT XGBOOST RF决策树系列的模型，一直是机器学习里非常重要，并用很广泛的模型。在这篇文章中，总结下决策树系列的模型的异同吧。">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树 GBDT XGBOOST RF">
<meta property="og:url" content="https://xinjieinformatik.github.io/2020/09/15/decision-tree/index.html">
<meta property="og:site_name" content="昕杰技术小站">
<meta property="og:description" content="决策树 GBDT XGBOOST RF决策树系列的模型，一直是机器学习里非常重要，并用很广泛的模型。在这篇文章中，总结下决策树系列的模型的异同吧。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xinjieinformatik.github.io/2020/09/15/decision-tree/20200713_220324_16.png">
<meta property="og:image" content="https://xinjieinformatik.github.io/2020/09/15/decision-tree/markdown-img-paste-20200819230120460.png">
<meta property="og:image" content="https://xinjieinformatik.github.io/2020/09/15/decision-tree/20200714_172640_17.png">
<meta property="og:image" content="https://xinjieinformatik.github.io/2020/09/15/decision-tree/20200714_174803_81.png">
<meta property="og:image" content="https://xinjieinformatik.github.io/2020/09/15/decision-tree/20200715_172149_44.png">
<meta property="article:published_time" content="2020-09-15T09:09:27.000Z">
<meta property="article:modified_time" content="2020-09-15T12:39:41.092Z">
<meta property="article:author" content="Xinjie">
<meta property="article:tag" content="决策树">
<meta property="article:tag" content="GBDT">
<meta property="article:tag" content="XGBOOST">
<meta property="article:tag" content="Random Forest">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xinjieinformatik.github.io/2020/09/15/decision-tree/20200713_220324_16.png">

<link rel="canonical" href="https://xinjieinformatik.github.io/2020/09/15/decision-tree/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>决策树 GBDT XGBOOST RF | 昕杰技术小站</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="昕杰技术小站" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/XinjieInformatik" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">昕杰技术小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">后端 机器学习</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  
 
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xinjieinformatik.github.io/2020/09/15/decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/dog.png">
      <meta itemprop="name" content="Xinjie">
      <meta itemprop="description" content="欢迎访问我的技术博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="昕杰技术小站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          决策树 GBDT XGBOOST RF
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-15 17:09:27 / 修改时间：20:39:41" itemprop="dateCreated datePublished" datetime="2020-09-15T17:09:27+08:00">2020-09-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="决策树-GBDT-XGBOOST-RF"><a href="#决策树-GBDT-XGBOOST-RF" class="headerlink" title="决策树 GBDT XGBOOST RF"></a>决策树 GBDT XGBOOST RF</h1><p>决策树系列的模型，一直是机器学习里非常重要，并用很广泛的模型。在这篇文章中，总结下决策树系列的模型的异同吧。</p>
<a id="more"></a>
<h2 id="决策树-ID3-C4-5-CART"><a href="#决策树-ID3-C4-5-CART" class="headerlink" title="决策树 ID3 C4.5 CART"></a>决策树 ID3 C4.5 CART</h2><p>决策树 是由多个基树（子树） 组成的，其中最重要的子树是 ID3, C4.5, CART 树，分别使用的是 信息增益，信息增益比 和 基尼系数 作为节点分裂的标准。ID3, C4.5是多叉树，CART树是二叉树。</p>
<p>信息熵, 代表系统的不确定性<br>$c_i$ 代表其中一个类别</p>
<script type="math/tex; mode=display">H(c) = -\sum_{i=1}^n p(c_i) log(p(c_i))</script><p>条件熵, 当特征X的分布被固定时系统的信息熵 H(c|X)</p>
<script type="math/tex; mode=display">H(c|X) = -\sum_{i=1}^n p(x_i)H(c|x=x_i)</script><script type="math/tex; mode=display">= -\sum_{i=1}^n  p(x_i)p(c|x=x_i) log(p(c|x=x_i))</script><script type="math/tex; mode=display">= -\sum_{i=1}^n p(c,x_i) log(p(c|x=x_i))</script><p>特征X的信息增益 (ID3使用)：特征X分布被固定时,系统不确定度的减小.<br>信息增益的大小是相对于训练数据集而言的,没有绝对意义,在数据集信息熵大的时候,信息熵也偏大.</p>
<script type="math/tex; mode=display">IG(c,X) = H(c) - H(c|X)</script><script type="math/tex; mode=display">= - \sum_{i=1}^n p(c_i) log(p(c_i)) + \sum_{i=1}^n p(c,x_i) log(p(c|x=x_i))</script><p>特征X的信息增益比 (C4.5使用)</p>
<script type="math/tex; mode=display">g = \frac{IG(c,X)}{H(X)} = \frac{H(c) - H(c|X)}{H(X)}</script><p>CART 基尼系数<br>可以做为熵模型的一个近似替代，避免耗时的对数运算</p>
<script type="math/tex; mode=display">Gini(p) = \sum_{i=1}^n p_i(1 - p_i)</script><p>互信息 两个随机变量之间的相关程度</p>
<p><img data-src="/2020/09/15/decision-tree/20200713_220324_16.png" alt="互信息"></p>
<p>KL散度 衡量两个分布的相似度</p>
<p><img data-src="/2020/09/15/decision-tree/markdown-img-paste-20200819230120460.png" alt="KL散度"></p>
<p>参考</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6050306.html">https://www.cnblogs.com/pinard/p/6050306.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6053344.html">https://www.cnblogs.com/pinard/p/6053344.html</a></li>
</ul>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益是相反的。</p>
<script type="math/tex; mode=display">Gini(p) = \sum_{k=1}^K p_k(1 - p_k)</script><h4 id="CART分类树对连续特征的处理"><a href="#CART分类树对连续特征的处理" class="headerlink" title="CART分类树对连续特征的处理"></a>CART分类树对连续特征的处理</h4><p>比如m个样本的连续特征A有m个，从小到大排列为𝑎1,𝑎2,…,𝑎𝑚,则CART算法取相邻两样本值的平均数，一共取得m-1个划分点。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。</p>
<h4 id="建立CART分类树"><a href="#建立CART分类树" class="headerlink" title="建立CART分类树"></a>建立CART分类树</h4><p>算法输入是训练集D，基尼系数的阈值，样本个数阈值。输出是决策树T。<br>我们的算法从根节点开始，用训练集递归的建立CART分类树。<br>1) 对于当前节点的数据集为D，如果样本个数小于阈值，return。<br>2) 计算样本集D的基尼系数，如果基尼系数小于阈值，return。<br>3) 计算当前节点的各个特征对数据集D的基尼系数.<br>4) 在各特征对数据集D的基尼系数中，选择基尼系数最小的特征A作为最优特征，把数据集划分成两部分D1和D2至左右节点。<br>5) 对左右的子节点递归的调用1-4步，生成决策树。</p>
<p>对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。</p>
<h4 id="建立CART回归树"><a href="#建立CART回归树" class="headerlink" title="建立CART回归树"></a>建立CART回归树</h4><p>CART回归树和CART分类树的建立和预测的区别主要有下面两点：<br>1) 连续值的处理方法不同<br>CART分类度量目标是树采用基尼系数最小,CART回归树的度量目标是均方差之和最小。<br>2) 决策树建立后做预测的方式不同。<br>CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p>
<h4 id="CART树建树时间复杂度"><a href="#CART树建树时间复杂度" class="headerlink" title="CART树建树时间复杂度"></a>CART树建树时间复杂度</h4><p>O(NMD), N是sample的大小，M是feature的数量，D是树的深度。cart生长时，把所有feature内的值都作为分裂候选，并为其计算一个评价指标（信息增益、增益比率、gini系数等），所以每层是O(NM)，D层的树就是O(NMD)</p>
<h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><p>决策树很容易出现的一种情况是过拟合(overfitting)，所以需要进行剪枝。而基本的策略包括两种：预剪枝(Pre-Pruning)与后剪枝(Post-Pruning)。</p>
<p>预剪枝：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。<br>后剪枝：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。<br>参考 <a target="_blank" rel="noopener" href="https://blog.csdn.net/bitcarmanlee/java/article/details/106824993">https://blog.csdn.net/bitcarmanlee/java/article/details/106824993</a></p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>不管是回归还是分类问题，GBDT使用的都是CART回归树, 不断基于负梯度拟合。损失函数除了loss还有正则项，正则中有参数和变量，很多情况下只拟合残差loss变小但是正则变大，代价函数不一定就小，这时候就要用梯度。 用残差去拟合，只是目标函数是MSE均方误差的一种特殊情况。 分类拟合的是概率的负梯度。<br>梯度提升树在函数空间做优化</p>
<p>梯度下降: 每次迭代基于负梯度方向更新权重参数<br>梯度提升: 每次迭代基于负梯度方向更新拟合函数(添加一个新函数)<br>对于新添加的函数取泰勒1阶/2阶近似,通过产生的一阶二阶梯度去拟合这个近似的函数.<br>最终权重参数/函数等于每次迭代增量的累加和.</p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost论文</a><br>这个文章xgboost总结的挺好: <a target="_blank" rel="noopener" href="https://snaildove.github.io/2018/10/02/get-started-XGBoost/">XGBoost总结</a></p>
<h4 id="GBDT-XGBoost区别"><a href="#GBDT-XGBoost区别" class="headerlink" title="GBDT, XGBoost区别"></a>GBDT, XGBoost区别</h4><p>对比原算法GBDT，XGBoost主要从下面三个方面做了优化：</p>
<ol>
<li>在算法的弱学习器模型选择上，GBDT只支持CART回归数，XGBoost还支持很多其他的弱学习器。GBDT的损失函数只对误差部分做负梯度一阶泰勒展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。</li>
<li>XGBoost决策树子树分裂特征选择的过程可并行，在连续的缓存中计算负梯度，解压缩时间换IO时间。</li>
<li>加入了L1和L2正则化项，列抽样,子采样,泛化能力更强。</li>
<li>对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。</li>
</ol>
<h4 id="XGBoost-并行"><a href="#XGBoost-并行" class="headerlink" title="XGBoost 并行"></a>XGBoost 并行</h4><ol>
<li>XGBoost在处理特征时可以做到并行处理，XGBoost并行原理体现在最优切分点的选择，假设样本数据共M个特征,分别对M个特征分别先预排序,然后以compressed column的形式存储在M个block中(减小内存占用), Block预先放入内存.在寻找每个特征的最优划分点的时候,可以利用多线程对每个block并行计算.</li>
<li>缓存感知:<br>CSC 存储格式会使得数据不在连续的内存里，用普通的访问方式会使得划分查找算法效率降低，并可能导致 cache miss。<br>对于精确贪婪算法，使用缓存感知cache-aware解决问题。首先为每个线程分配一个内部buffer，读取梯度信息并存入buufer中(实现非连续到连续的转化), 然后再在buffer中统计梯度信息。<br>对于近似算法，选取正确的 block 大小就可以解决问题。</li>
<li>核外块的计算<br>为了解决硬盘读取数据耗时长,吞吐量不足.<br>多线程对数据分块压缩(block compression)存储在硬盘上,再将数据传输到内存,最后再用独立的线程解压缩. (用解压缩时间换取IO时间)</li>
</ol>
<p>参考:<a target="_blank" rel="noopener" href="https://blog.rocuku.cc/xgboost-summary/">https://blog.rocuku.cc/xgboost-summary/</a><br><a target="_blank" rel="noopener" href="https://snaildove.github.io/2018/10/02/get-started-XGBoost/">https://snaildove.github.io/2018/10/02/get-started-XGBoost/</a></p>
<h4 id="XGBoost-损失函数"><a href="#XGBoost-损失函数" class="headerlink" title="XGBoost 损失函数"></a>XGBoost 损失函数</h4><p><img data-src="/2020/09/15/decision-tree/20200714_172640_17.png" alt="20200714_172640_17"><br>L 为损失函数, Omiga为树的复杂度, T 为叶子结点个数，w 为叶子权重。<br><img data-src="/2020/09/15/decision-tree/20200714_174803_81.png" alt="20200714_174803_81"><br>节点分裂公式<br><img data-src="/2020/09/15/decision-tree/20200715_172149_44.png" alt="20200715_172149_44"><br>损失函数本身满足样本之间的累加特性(最大似然取log的好处)，所以，可以通过将分裂前的叶结点上样本的损失函数和与分裂之后的两个新叶结点上的样本的损失函数之和进行对比结合复杂度的控制,计算增益gain，寻找最优特征与最优分割点。</p>
<h4 id="XGBoost防止过拟合的方法"><a href="#XGBoost防止过拟合的方法" class="headerlink" title="XGBoost防止过拟合的方法"></a>XGBoost防止过拟合的方法</h4><ul>
<li>目标函数添加正则项：叶子节点个数+叶子节点权重的L1,L2正则化</li>
<li>列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）</li>
<li>子采样：每轮计算可以不使用全部样本，使算法更加保守</li>
<li>ealry stop</li>
<li>剪枝</li>
</ul>
<p>这个文章讲的比较好: <a target="_blank" rel="noopener" href="https://www.xuyindavid.top/2019/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98/xgboost/%E7%99%BE%E9%97%AExgboost/">百问XGBoost</a></p>
<h4 id="xgboost-特征重要性筛选"><a href="#xgboost-特征重要性筛选" class="headerlink" title="xgboost 特征重要性筛选"></a>xgboost 特征重要性筛选</h4><p>xgboost实现中Booster类get_score方法输出特征重要性，其中importance_type参数支持三种特征重要性的计算方法：</p>
<ol>
<li>importance_type=weight（默认值），特征重要性使用特征在所有树中作为划分属性的次数。</li>
<li>importance_type=gain，特征重要性使用特征在作为划分属性时loss平均的降低量。</li>
<li>importance_type=cover，特征重要性使用特征在作为划分属性时对样本的覆盖度。</li>
</ol>
<h4 id="不需要归一化原因"><a href="#不需要归一化原因" class="headerlink" title="不需要归一化原因"></a>不需要归一化原因</h4><p>增益gain的计算与特征值范围无关,是采用生成树的结构与权重计算的,所以不需要对特征进行归一化处理.</p>
<h4 id="XGBoost如何处理缺失值"><a href="#XGBoost如何处理缺失值" class="headerlink" title="XGBoost如何处理缺失值"></a>XGBoost如何处理缺失值</h4><p>XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：</p>
<ul>
<li>在特征k非缺失的样本上上寻找最佳切分点。</li>
<li>将该特征值缺失的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个分支，作为预测时特征值缺失样本的默认分支方向。</li>
<li>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。</li>
</ul>
<h4 id="XGBoost中的一棵树的停止生长条件"><a href="#XGBoost中的一棵树的停止生长条件" class="headerlink" title="XGBoost中的一棵树的停止生长条件"></a>XGBoost中的一棵树的停止生长条件</h4><ul>
<li>当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。</li>
<li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。</li>
<li>如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</li>
<li>验证集上预剪枝</li>
</ul>
<h4 id="最优切分点划分算法"><a href="#最优切分点划分算法" class="headerlink" title="最优切分点划分算法"></a>最优切分点划分算法</h4><ul>
<li>对每个叶节点枚举所有的可用特征</li>
<li>针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，从左到右线性扫描决定该特征的最佳分裂点，并记录该特征的分裂增益(左孩子损失+右孩子损失-分裂前损失+复杂度)</li>
<li>选择增益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集<br>每次排序的时间复杂度 O(m nlogn), m为特征类别数, n为样本数. 可以通过预排序节省.</li>
</ul>
<p>近似算法: 上述算法可以得到最优解，但计算量大. 对于每个特征，只考察分位点可以减少计算复杂度.<br>根据分位数采样得到分割点的候选集合S, 将特征k的值根据集合Sk划分到桶(bucket)中，接着对每个桶内样本的增益(梯度G,H)进行累加统计，最后在这些累计的统计量上寻找最佳分裂点。<br>除此之外还有带权重(基于梯度)的分位方案.</p>
<p>参考: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/105612830">https://zhuanlan.zhihu.com/p/105612830</a><br><a target="_blank" rel="noopener" href="https://yxzf.github.io/2017/04/xgboost-v2/">https://yxzf.github.io/2017/04/xgboost-v2/</a></p>
<h3 id="随机森林-RF"><a href="#随机森林-RF" class="headerlink" title="随机森林 RF"></a>随机森林 RF</h3><p>使用bagging的方法，每次对数据集有放回的随机采样，使用一部分数据训练不同的基学习器，最后再集成起来.<br>随机森林使用CART作为基学习器,不同的是,RF随机选择节点上的一部分特征𝑛𝑠𝑢𝑏，然后在这些随机选择的𝑛𝑠𝑢𝑏个样本特征中，选择一个最优的特征来做决策树的左右子树划分。(nsub越小,模型方差越小,偏差越大.一般通过交叉验证调参获取合适的𝑛𝑠𝑢𝑏) 也可以随机混合使用ID3 C4.5 CART.<br>如果是分类，则T个弱学习器投出最多票数的类别为最终类别。如果是回归，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。<br>优点:</p>
<ol>
<li>训练与预测过程可以高度并行化</li>
<li>采用随机采样，随机选取节点特征,训练出的模型的方差小，泛化能力强。</li>
</ol>
<h4 id="RF和GBDT的区别"><a href="#RF和GBDT的区别" class="headerlink" title="RF和GBDT的区别"></a>RF和GBDT的区别</h4><p>相同点：</p>
<ul>
<li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li>
</ul>
<p>不同点：</p>
<ul>
<li>集成学习：RF属于bagging思想，而GBDT是boosting思想</li>
<li>偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差</li>
<li>训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本</li>
<li>并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)</li>
<li>最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合</li>
<li>数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>泛化能力：RF不易过拟合，而GBDT容易过拟合</li>
</ul>
<h3 id="LightGBM和XGBoost的区别？"><a href="#LightGBM和XGBoost的区别？" class="headerlink" title="LightGBM和XGBoost的区别？"></a>LightGBM和XGBoost的区别？</h3><p><a target="_blank" rel="noopener" href="https://my.oschina.net/u/4314328/blog/3338894">https://my.oschina.net/u/4314328/blog/3338894</a></p>
<h3 id="LR和GBDT比较，什么情景下GBDT不如LR"><a href="#LR和GBDT比较，什么情景下GBDT不如LR" class="headerlink" title="LR和GBDT比较，什么情景下GBDT不如LR"></a>LR和GBDT比较，什么情景下GBDT不如LR</h3><p>逻辑回归LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程。GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练并且很容易过拟合。</p>
<p>先看一个例子：<br>假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。</p>
<ul>
<li>在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，过拟合。</li>
<li>如果采用LR的话，也会出现过拟合的现象，但可以通过正则化更有效的克制。<br>LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值。树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而示例，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项很小。</li>
</ul>
<p>在高维稀疏特征的时候：带正则化的线性模型比较不容易对稀疏特征过拟合。</p>

    </div>

    
    
    
      

        <div class="reward-container">
  <div>技术分享</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="Xinjie 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Xinjie
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://xinjieinformatik.github.io/2020/09/15/decision-tree/" title="决策树 GBDT XGBOOST RF">https://xinjieinformatik.github.io/2020/09/15/decision-tree/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="tag"># 决策树</a>
              <a href="/tags/GBDT/" rel="tag"># GBDT</a>
              <a href="/tags/XGBOOST/" rel="tag"># XGBOOST</a>
              <a href="/tags/Random-Forest/" rel="tag"># Random Forest</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/09/14/sql-base/" rel="prev" title="SQL基础">
      <i class="fa fa-chevron-left"></i> SQL基础
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  
 


          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
       
      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树-GBDT-XGBOOST-RF"><span class="nav-text">决策树 GBDT XGBOOST RF</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树-ID3-C4-5-CART"><span class="nav-text">决策树 ID3 C4.5 CART</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CART"><span class="nav-text">CART</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CART分类树对连续特征的处理"><span class="nav-text">CART分类树对连续特征的处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#建立CART分类树"><span class="nav-text">建立CART分类树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#建立CART回归树"><span class="nav-text">建立CART回归树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART树建树时间复杂度"><span class="nav-text">CART树建树时间复杂度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#剪枝"><span class="nav-text">剪枝</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost"><span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT-XGBoost区别"><span class="nav-text">GBDT, XGBoost区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost-并行"><span class="nav-text">XGBoost 并行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost-损失函数"><span class="nav-text">XGBoost 损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost防止过拟合的方法"><span class="nav-text">XGBoost防止过拟合的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xgboost-特征重要性筛选"><span class="nav-text">xgboost 特征重要性筛选</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#不需要归一化原因"><span class="nav-text">不需要归一化原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost如何处理缺失值"><span class="nav-text">XGBoost如何处理缺失值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost中的一棵树的停止生长条件"><span class="nav-text">XGBoost中的一棵树的停止生长条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最优切分点划分算法"><span class="nav-text">最优切分点划分算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林-RF"><span class="nav-text">随机森林 RF</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RF和GBDT的区别"><span class="nav-text">RF和GBDT的区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM和XGBoost的区别？"><span class="nav-text">LightGBM和XGBoost的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LR和GBDT比较，什么情景下GBDT不如LR"><span class="nav-text">LR和GBDT比较，什么情景下GBDT不如LR</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xinjie"
      src="/images/dog.png">
  <p class="site-author-name" itemprop="name">Xinjie</p>
  <div class="site-description" itemprop="description">欢迎访问我的技术博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/XinjieInformatik" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;XinjieInformatik" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>



      </div>
      
      <!--
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=427606780&auto=1&height=66"></iframe>
      -->
      <div id="">
      <canvas id="canvas" style="width:60%;">
      </div>
      <script type="text/javascript" src="/js/canvas-dance-time.js"></script>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>
  



      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div id="days"></div>
<script>
function show_date_time(){
    window.setTimeout("show_date_time()", 1000);
    BirthDay=new Date("09/09/2020 08:13:14");//修改为自己的blog建站时间
    today=new Date();
    timeold=(today.getTime()-BirthDay.getTime());
    sectimeold=timeold/1000
    secondsold=Math.floor(sectimeold);
    msPerDay=24*60*60*1000
    e_daysold=timeold/msPerDay
    daysold=Math.floor(e_daysold);
    e_hrsold=(e_daysold-daysold)*24;
    hrsold=setzero(Math.floor(e_hrsold));
    e_minsold=(e_hrsold-hrsold)*60;
    minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
    seconds=setzero(Math.floor((e_minsold-minsold)*60));
    document.getElementById('days').innerHTML="本站已安全运行"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒";
}
function setzero(i){
    if (i<10)
    {i="0" + i};
    return i;
}
show_date_time();
</script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xinjie</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">40k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">36 分钟</span>
</div>
<!--
--!>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://xinjieinformatik.github.io/2020/09/15/decision-tree/',]
      });
      });
  </script>
 
  

  <script async src="/js/cursor.js"></script> 

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'a054acd527b7f50a7920',
      clientSecret: '2f3218b7d2f9dd1ed5ccd36593a3572a9c4d28d3',
      repo        : 'gitalks_comments',
      owner       : 'XinjieInformatik',
      admin       : ['XinjieInformatik'],
      id          : '4ae25689b36d2666876272c77817d988',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
